[{"path":"https://tlyons253.github.io/MDChelp/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 MDChelp authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Bootstrapping.html","id":"linear-model-basics","dir":"Articles","previous_headings":"","what":"Linear Model Basics","title":"Non-parametric Bootstrapping","text":"fit model data, making assumptions data, typically (limited ) something data distributed terms model related response (e.g., linear). can assumptions examples : Typically used continuous variables Model terms response linearly related residuals normally distributed homogeneous variance. Typically used model count data Model terms log response linear related mean response equal variance (Poisson) assumptions sometimes met important negative consequences results. John Fieberg’s book Statistics Ecologists goes details, ’s bad. long short : violate linearity assumption, regression coefficients biased. Think left quadratic term linear model e.g. \\[Y=\\beta_0+\\beta_1\\cdot x_1\\\\ \\\\ vs\\\\ \\\\ Y=\\beta_0+\\beta_1\\cdot x_1 + \\beta_2\\cdot (x_1)^2\\] Using just linear term \\((\\beta_1)\\) accurately describe relationship \\(Y\\) \\(x_1\\) \\(\\beta_1\\) biased. Another common area can issue use covariate areal measurement count something else verrrry broad scale (values 10’s 1,000’s ). issue isn’t nature variable per se, fact scale , broad. also leads response covariates linearly related. assumptions model/data aren’t met, normal residuals LM mean-equal--variance Poisson, sampling distribution \\(\\beta\\) underestimated. means reported SE particular \\(\\beta\\) small resulting confidence interval p-value incorrect. violate assumptions? linearity assumption supported, change type model (assumed underlying distribution) perform kind transformation ensures linear relationship met. case covariates broad ranges, may log transform covariate. Often clear issues, assumptions variances residuals may met. example, Poisson regression, common type violation greater variance expected, data overdispersed. practice, overdispersion can arise omit kind grouping variable (e.g. ignoring potential correlated survival probability gamebird chicks brood), 0’s expected. fit model ignored factors, can longer trust SE’s , CI’s, p-values \\(\\beta's\\) output typical routines (e.g. glm R; See section Dr. Fieberg’s book -depth discussion issues CI’s p-values reported common software functions) might find goodness--fit (GOF) test indicate model poor fit. simple LM, may find matter transformation use, terms add, still don’t normal residuals. fix problem include random effect grouping variable (brood first case), change distribution, like switching negative binomial distribution second case. even specify model correctly, try different distributions, may find model pass GOF test. ? Bootstrap data.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Bootstrapping.html","id":"how-to-bootstrap","dir":"Articles","previous_headings":"","what":"How to bootstrap","title":"Non-parametric Bootstrapping","text":"Bootstrapping comes 3 flavors (parametric, semi-parametric, non-parametric) non-parametric bootstrapping discussed consequently everything described may hold bootstrap alternatives. case non-parametric bootstrap, relax distributional assumptions model (mean/ variance relationship, residuals) instead make different assumptions, namely resampling scheme matches original sampling scheme. still deal issues like linearity, independence, etc. either model /via resampling method. assumptions can still met, bootstrap allows estimate distribution parameters, regression coefficients (\\(\\beta's\\)) model-based estimates predictions. short version , can use bootstrap estimate SE parameter. work? brief version : Resample data replacement Estimate model parameters, save parameters Repeat thousand times N estimates parameter. N estimates comprise approximation distribution parameter. don’t use estimate parameter, SE/ CI parameter instead. USE DISTRIBUTION ESTIMATE PARAMETER , SE/CI. best estimate parameter mean comes model using original data.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Bootstrapping.html","id":"determining-the-resampling-scheme","dir":"Articles","previous_headings":"How to bootstrap","what":"Determining the resampling scheme","title":"Non-parametric Bootstrapping","text":"validity bootstrap dependent resampling scheme matching manner data collected, preserving random structures sampling process. easy observations randomly sampled. can just resample data, , random. Like code creates 10 bootstrap data sets original data,tmp. print boot.dat, see 10 different data frames. can see row labels identify observations comprise new data. observation original data sampled , gets decimal value. example, row labeled 5.3 3rd appearance bootstrapped data observation 5th row original data. example, 10 bootstrapped data sets, reality, probably want minimum 2,000. fact, resampling scheme complex, likely need 2,000; possibly many 10,000. resampling gets complicated start data naturally falls clusters. something account model, fixed random effect. important part , ’s something arises randomly observation (.e. randomly sampling people observing eye color), ’s part study design (randomly sampling 5 people blue eyes, 5 brown, etc.) cluster/variable fixed effect part study design, resample level fixed effect, need accounted resampling scheme. variable treated random effect, get resampled. example, think study test scores collected students, 5 classrooms per school, 4 schools: Schools aren’t randomly selected, classrooms within school randomly selected, 20 students within classroom measured, Resample 5 classrooms within school, replacement resample students within classrooms. resample schools. scenario, classrooms randomly selected randomly sampled. Every school still represented 100 students school, number different classrooms differs, leading classrooms accounting 20 students. 10 students randomly sampled classroom instead 20? Now just make sure students also resampled within classroom. Go inspect boot.10 . Now, rather student appearing time classroom appears bootstrap, student may appear >1 time time classroom selected. still 5 classrooms per school 10 observations per school/classroom, ’s just now appear . previously, student appear bootstrapped data set 5 times (assuming classroom randomly selected 5x). Now, theory, student appear many 50 times (classroom selected 5 times, selected 10 times, 5x). can adjust sampling scheme allowing random sampling different levels, combining variables construct appropriate “cluster” perform resampling . Just remember, randomly selected study design, needs randomly sampled bootstrap.","code":"data.frame(X1=seq(1,10,1),            X2=rnorm(10,0,1))%>%   mutate(Y=0.2*X1+X2)%>%   select(-X2)->tmp  #Create 10 bootstrapped data sets  purrr::map(1:10,~sample(1:nrow(tmp),size=nrow(tmp),replace=TRUE)%>%               tmp[.,])->boot.dat  boot.dat library(tidyverse) demo.dat<-expand.grid(school=c(\"Hogwarts\",                                \"Tattoine P.S. 1\",                                \"Hobbition Elementary\",                                 \"Mordor Primary\"),                       room=seq(1:5),                       student=seq(1:20)                       )%>%   mutate(score=sample(50:100,nrow(.),replace=TRUE),          room=paste0(school,'_',room),          student_id=row_number())  # create 1 bootstrap data sets by creating indices of the data selected. Run the code block, line by line (cumulatively), to see what is being done at each step. demo.dat%>%   distinct(school, room)%>%   split(.$school)%>%   map(.,~slice_sample(.x,n=nrow(.x),replace=TRUE))%>%   bind_rows()%>%   purrr::pmap(.,~filter(demo.dat,                        school=={..1},                        room=={..2}))%>%   bind_rows()  #create 10 data sets   map(c(1:10),~demo.dat%>%   distinct(school, room)%>%   split(.$school)%>%   map(.,~slice_sample(.x,n=nrow(.x),replace=TRUE))%>%   bind_rows()%>%   purrr::pmap(.,~filter(demo.dat,                        school=={..1},                        room=={..2}))%>%   bind_rows())->boot.10  map(boot.10,~.x%>%       group_by(school)%>%       tally())  # number of students from each school  map(boot.10,~.x%>%       group_by(school,room)%>%       tally())  # number of students from each school gdata::keep(demo.dat,sure=TRUE) # make an \"original\" data set of 10 students from each classroom. We are pretending the other 10 students were never measured. You won't do this step ever with your data demo.dat<-expand.grid(school=c(\"Hogwarts\",                                \"Tattoine P.S. 1\",                                \"Hobbition Elementary\",                                 \"Mordor Primary\"),                       room=seq(1:5),                       student=seq(1:10) #This is now 10 students                       )%>%   mutate(score=sample(50:100,nrow(.),replace=TRUE),          room=paste0(school,'_',room),          student_id=row_number())   # create one bootstrap data set ---------------------------------------------   # create indexes of randomly sampled classrooms (within schools) and students   demo.dat%>%   distinct(school, room)%>%   split(.$school)%>%   map(.,~slice_sample(.x,n=nrow(.x),replace=TRUE))%>%     bind_rows()->school.room.idx # list of resampled classrooms    #use the list of resampled classrooms to resample students within      pmap(school.room.idx,~filter(demo.dat,                                school=={..1},                                room=={..2}))%>%     map(.,~slice_sample(.x,n=nrow(.x),replace=TRUE))           # List is 20 elements long, with 5 elements per school. Room ID's may be # repeated within a school and student_id can be repeated within a room.     # make 10 data sets where classrooms in schools and students in classrooms are randomly resampled      map(c(1:10),~ demo.dat %>%     distinct(school, room) %>%     split(.$school) %>%     map(.,  ~ slice_sample(.x, n = nrow(.x), replace = TRUE)) %>%     bind_rows() %>%     pmap(.,  ~ filter(demo.dat,                       school == {..1},                       room == {..2})) %>%     map(.,  ~ slice_sample(.x, n = nrow(.x), replace = TRUE)) %>%     bind_rows())->boot.10"},{"path":"https://tlyons253.github.io/MDChelp/articles/Bootstrapping.html","id":"deriving-se-and-cis","dir":"Articles","previous_headings":"How to bootstrap","what":"Deriving SE and CI’s","title":"Non-parametric Bootstrapping","text":"bootstrap estimates, make density plot histogram determine distribution parameter normal close enough. Close enough counts thousands estimates. looks normal, can compute SE bootstrap estimates (e.g. sd(boostrap_estimates$Var1->SE) compute 95% (%) confidence interval \\(\\mu\\ \\pm\\ 1.96\\times\\ SE\\) \\(\\mu\\) estimate model using original data. shows skew (likely , even ’s just fat tails), need use form quantile (e.g.quantile(SE,c(0.025,0.975)). , however, derive CI simple call quantile. interval get might include mean obtained using original data. bcaboot package perform , handles steps bootstrap process resampling data computing statistics. unless statistics ’re straightforward (resampling scheme simple) may able utilize . Another version exists thecoxed package via coxed::bca(). use . appropriate non-parametric bootstrap. MDChelp package functions compute BCa interval given table bootstrapped estimates original data (MDChelp::bca_jacknife). Right now, ’s tested creating table regression parameter estimates CI’s \\((\\beta's)\\) might able get model-based estimates (e.g. marginal means, contrasts, etc.) depending identify parameter interest table function provide.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Bootstrapping.html","id":"deriving-p-values","dir":"Articles","previous_headings":"How to bootstrap","what":"Deriving P-values","title":"Non-parametric Bootstrapping","text":"typically don’t estimate p-value bootstrap. , need permutation test. inference relies reporting interpreting p-values: Don’t. Use permuation test instead. “approximations” can use calculating T (presumably F) statistics, , seems odds whole purpose using non-parametric bootstrap.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Bootstrapping.html","id":"example-workflow","dir":"Articles","previous_headings":"","what":"Example Workflow","title":"Non-parametric Bootstrapping","text":"real worked example includes tips save computational speed/ memory.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Bootstrapping.html","id":"computational-tips","dir":"Articles","previous_headings":"Example Workflow","what":"Computational tips","title":"Non-parametric Bootstrapping","text":"real bootstrap, can take advantage parallel processing avoid running memory limits writing bootstrapped data file hard drive. makes easy use furrr purrr execute analysis organize results.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Bootstrapping.html","id":"create-bootstrapped-data","dir":"Articles","previous_headings":"Example Workflow","what":"Create bootstrapped data","title":"Non-parametric Bootstrapping","text":"Now data written folder specified folder path. may may memory savings generating data executing parallel, almost certainly running analysis. Parallel processing easy furrr package case, split data objects just created among however many cores/workers specify. means might helpful math ahead time figure many cores want assign. mid-range computers now 8 ‘logical’ cores (4 physical) can use (assuming Windows OS). need reserve least 1 handle processes. Beyond ’s . generally try use number cores results number data objects assigned core. clue actually helps hurts, ’s …","code":"#create a set of dummy data   data.frame(X=rnorm(100,0,5),             G=rep(c(1,0),each=50),             noise=rnorm(100,0,1)            )%>%   mutate(Y=1+2*G+0.2*X+noise)%>%   select(-noise)->tmp  folder.name<-\"Yourpath/boot_data/\" #complete path to a folder to hold bootstrapped data. Create this folder ahead of time  #assume G is part of the sampling design, but is not random so observations must be randomly sampled within each level of G  # write the function to do the bootstrapping and write to file, given just the original data and a counter, i. Essentially write a function that resamples the data appropriately, once.   make.bootdat<-function(X,i){   X%>%     split(.$G)%>%     map(.,~slice_sample(.x,n=nrow(.x), replace=TRUE))%>%     bind_rows()->out      saveRDS(out,file=paste0(folder.name,'bootdat_',i,'.rds'))    #always save RDS, use the counter i to name each replicate data set    }  # now use purrr:: walk to execute the above function multiple times  purrr::walk(c(1:100),~make.bootdat(tmp,.x))"},{"path":"https://tlyons253.github.io/MDChelp/articles/Bootstrapping.html","id":"create-model-objects","dir":"Articles","previous_headings":"Example Workflow","what":"Create model objects","title":"Non-parametric Bootstrapping","text":"can use purrr::map process model object , depending want . Just remember, want parallel,need reading/writing file, memory. desired result (maybe just model object), can store memory. simplicity , just assume want just get table regression coefficients BCa intervals. ’ll use model objects stored workspace, tweak needed made function read data first, like reg.fxn.2().","code":"#create a list of file paths for all the bootstrapped data  bootdat.list<-list.files(path='your path/boot_data/',                          full.names=TRUE,                          pattern='.rds')  #write a function that will take the file path, read in that data object, do the desired analysis. you can make it write to memory/ workspace, or to a folder again. I like to write the model object (if using a model) at this stage because then you have more flexibility later on to get regression coefficents or compute contrasts/ marginal means without needing to re-run the model   reg.fxn.1<-function(X){   readRDS(file=X)->boot.dat      lm(Y~X+G,data=boot.dat)->out      return(out) }  #same as above, but writes to file. Needs a counter, i  reg.fxn.2<-function(X,i){   readRDS(file=X)->boot.dat      lm(Y~X+G,data=boot.dat)->out      saveRDS(out,file=paste0('Yourpath/boot_mods/model_',i,'.rds')) }   #non-parallel, write results to memory/workspace  map(bootdat.list,~reg.fxn.1(.x))->coef.tab.list     # set up the parallel processing structure only use if you write the objects to a file, not to memory.  future::plan(strategy='multisession',              workers=5) # use walk2 because we are writing to a file, and create the counter vector as a second argument for walk2  furrr::future_walk2(bootdat.list,                     c(1:length(bootdat.list)),                     ~reg.fxn.2(.x,.y))   future::plan(strategy='sequential') #turn off parallel processing"},{"path":"https://tlyons253.github.io/MDChelp/articles/Bootstrapping.html","id":"summarize-estimates-and-compute-bca-intervals","dir":"Articles","previous_headings":"Example Workflow","what":"Summarize estimates and compute BCa intervals","title":"Non-parametric Bootstrapping","text":"MDChelp::bca_jacknife() function needs table named parameters bootstrapped estimates. easy obtain using broom::tidy(). wants column represents parameter value called “theta.boot”. also need pass function replicates analysis step previously-whatever wanted original data make match format bootstrapped estimates (e.g. reg.fxn ). may need slight tweaks doesn’t write folder. Alpha sets error rate multcomp determines whether family-wide error rate (testing whether parameters = 0) per-variable basis. uses Sidak method correction. intended used returning table regression coefficients. something returning table regression coefficients (e.g. using emmeans, ggeffects), set multcomp=FALSE.","code":"map(coef.tab.list,~broom::tidy(.x)%>%       select(term,              theta.boot=estimate))%>% #rename the value to theta.boot   bind_rows()->boot.param.tab # bind rows to make it all one data frame  #function to analyze the original data, similar to reg.fxn above. bca.fxn<-function(X){   lm(Y~G+X,data=X)%>%     broom::tidy(.)%>%     select(term, estimate)->out      return(out) }    MDChelp::bca_jacknife(boot.param.tab,tmp,bca.fxn,alpha=0.05,multcomp=FALSE)"},{"path":"https://tlyons253.github.io/MDChelp/articles/Capture-Recapture-Abundance-Estimation--Marked Individuals.html","id":"closed-population-abundance-estimation-methods--marked-individuals","dir":"Articles","previous_headings":"","what":"Closed population abundance estimation methods- marked individuals","title":"Capture-Recapture: Abundance Estimation Closed Populations- Marked Individuals","text":"several common abundance estimation methods suitable closed populations. Demographic closure means , animals observed, births, deaths, immigration, emigration within study area. methods identified appropriate “marked” individuals. , assume physically capturing marking individuals. Another document covers abundance estimation closed populations individuals can uniquely identify, physically marking . methods various formulations, different assumptions, constraints, data collection methods, solving equation. \\[ N=\\frac{n}{p} \\] \\(N\\) number individuals, \\(n\\) number individuals observe, \\(p\\) “encounter” (capture recapture) probability. thing need retain equation methods trying use different approaches estimate \\(p\\). interested understanding math, discussion well. Lincoln-Petersen Method Closed-capture likelihood models variations Removal Models original Lincoln-Petersen (LP) estimator (Petersen 18961, Lincoln 19302) simple 2-sample technique estimate abundance. 2-sample technique, mean need 2 capture occasions: first capture occasion mark release animals, additional occasion recapture . ’s . LP estimator calculates abundance : \\[N=\\frac{n_1\\times \\ n_2}{k}\\] \\(n_1\\) number animals encountered first occasion. encountered animals marked released. \\(n_2\\) total number animals encountered second occasion. \\(k\\) number marked individuals \\(n_1\\), observed \\(n_2\\). simple equation, ’s one actually used practice. end using LP, likely use Chapman version/ estimator. ’s ’s implemented MDChelp::chapman(). Multiple variations extensions LP method exist. multiple encounters mark, release, recapture individuals, alternative known Schnabel method/estimator appropriate (Zoe Emily Schnabel). also methods combine Schnabel method simultaneous removals, discussed . extensions beyond simple LP framework largely supplanted methods use uniquely marked animals, primarily development better marking techniques (e.g. RFID tags). Still, ’s important highlight conceptual contributions individuals. ’s also helpful reminder less common methods sill viable, can’t meet data collection methods contemporary alternatives. times LP approach won’t cut . Usually ’s assumption homogeneous encounter probability marked unmarked animals isn’t met. Alternatives LP (Scnabel) still rely demographic closure assumptions, can account heterogeneous encounter probabilities among individuals across encounter events. requires changes. Mathematically, now start using likelihood based methods. just means start using probability theory distributions estimate abundance rather algebra like . practical, real-world implication , now need individuals marked unique markings need follow . lot variations using likelihood based approach (see Chapter 14 Program MARK “Gentle Introduction”) first difference aware using “full-likelihood” “conditional” likelihood. mentioned inarticle(\"Capture-Recapture-Overview\",package=\"MDChelp\")","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Capture-Recapture-Abundance-Estimation--Marked Individuals.html","id":"important-assumptions-limitations-tidbits","dir":"Articles","previous_headings":"Closed population abundance estimation methods- marked individuals","what":"Important Assumptions/ Limitations/ Tidbits","title":"Capture-Recapture: Abundance Estimation Closed Populations- Marked Individuals","text":"population closed first second capture events!!!! Tags markings need unique among individuals, tag losses following release marked individuals. individuals recapture probability. capture probability first event can different probability second event, capture probability unmarked marked individuals . means methods used capture individuals first event marking can different used capture individuals individuals. extreme example LP method used estimate abundance initial capture period (obviously) live capture, marking release, recapture dead recovery (e.g. waterfowl band recovery, tagged fish). Performs poorly fewer 50 individuals ( ideally, mark > 100) \\(p\\) small. important group-level heterogeneity capture probability. capture probability differs male/ female adult/juvenile, “bin” individuals group separately can lead small \\(n_1\\) group","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Capture-Recapture-Abundance-Estimation--Marked Individuals.html","id":"why-still-use-it","dir":"Articles","previous_headings":"Closed population abundance estimation methods- marked individuals","what":"Why still use it ?","title":"Capture-Recapture: Abundance Estimation Closed Populations- Marked Individuals","text":"“better” methods exist deal common violations assumptions LP relies (capture heterogeneity), typically require find populations sample, capture events, etc. fact alternatives might difficult implement excuse just use LP adequately meet assumptions!!! worth remembering though, can advantageous alter study design meet assumptions simpler approach address logistical animal welfare constraints methods flexible relaxed assumptions.","code":""},{"path":[]},{"path":[]},{"path":"https://tlyons253.github.io/MDChelp/articles/Capture-Recapture-Overview.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Capture-Recapture: Overview","text":"Capture-recapture methods related concepts foundation plethora methods analytic techniques used ecological. can difficult know best design study even type data collect without first kind idea analysis might perform. goal notes provide basic overview can capture-recapture methods, basic data needs assumptions, provide basic tools help guide development projects. many, many books cover topic much better clarity detail, three may helpful particular : Estimation Parameters Animal Populations (Powell Gale 2015) Population Ecology Practice (Murray Sandercock 2022) Program Mark “Gentle Introduction” offer comprehensive discussions specific types models without overly technical (first two), though “Gentle Introduction” goes much discussion given model, also giving guidance implementation Program MARK. related articles meant quick reference, help make aware way comprehensive enough resource use. definitely read relevant sections books, papers, consult one biometricians identify technique interested using project.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Capture-Recapture-Overview.html","id":"key-terminology-and-concepts","dir":"Articles","previous_headings":"","what":"Key Terminology and Concepts","title":"Capture-Recapture: Overview","text":"characteristics/concepts help frame/organize/differentiate among plethora methods serve basis critical assumptions. looking specific methods models, ’ll see terms referenced frequently.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Capture-Recapture-Overview.html","id":"model-assumptions-open-or-closed-populations","dir":"Articles","previous_headings":"Key Terminology and Concepts","what":"Model assumptions: open or closed populations","title":"Capture-Recapture: Overview","text":"Whether population can classified open closed defines capture-recapture methods critical assumption different models. Closed simpler define, different processes makes population “open”. simply, closed population means , time observing animals, births, deaths, immigration emigration kind. animals () observe capture individuals change. births, deaths, movement study area exist, data collection, open population. ’s important concept understand, always black white practice. lines “open” “closed” populations (related statistical models) blur processes relevant particular model (deaths emigration, Cormack-Jolly-Seber), structure data collection define time frame. Example: go capture mark birds field estimate abundance year 5 years, clearly birds. years, even within year, birds can born, can move field, leave field, die. However, time marking birds given year, none processes occur, , purposes data collection analysis, closed population. can use model assumes closed population analyze data year. Thus, depending interested measuring design data collection, can sometimes treat truly “open” population closed. Example: Cormack-Jolly-Seber (CJS) model considered “open” population model expect individuals die estimate apparent survival individuals encounters. ’s called apparent survival also individuals leave study area (emigrate), can’t differentiate deaths leaving. However, can safely assume individuals don’t leave, population open (individuals can still die), estimating true survival. Movement population births don’t matter instance, CJS model tries estimate apparent survival. able think concept open closed populations relates study organism, question, data collection key backbone virtually every statistical model use acts important building block, permitting use complex models answer complex questions.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Capture-Recapture-Overview.html","id":"conditioning-on-initial-capture-or-modeling-the-initial-capture-full-capture-history","dir":"Articles","previous_headings":"Key Terminology and Concepts > Model assumptions: open or closed populations","what":"Conditioning on initial capture or modeling the initial capture (full capture history)","title":"Capture-Recapture: Overview","text":"less important understand detail stage, ’s important technical concept applies open-population models. Conditioning initial capture just means don’t particularly care modeling initial capture animal, care happens caught released . Methods like Cormack-Jolly-Seber ignore initial capture process try estimate apparent survival recapture probability. models try explicitly model initial capture process (e.g. Jolly-Seber, Pradel, POPAN, closed-capture). approach get ? explicitly trying model initial capture, can also start estimate parameters like abundance population growth rate. condition initial capture, skip focus happens animals afterwards, typically focusing apparent survival movement. always model initial capture gets information animal population? Logistically, can challenging. Modeling initial capture requires constantly capturing marking animals throughout time. ’s easy support kind effort result. Full capture history methods might work well studies last years animals captured marked occasions, relative frequently may encountered (e.g. waterfowl banding, translocation studies). also need method initial capture subsequent encounters. Finally, estimates parameters (like abundance) might good. closed-capture models, lot focus dealing issues related heterogeneity capture probability, whether ’s trap-response, age/cohort/time differences, etc. complexity doesn’t go away, probably gets worse open models. can’t deal open models like can closed models (see “robust design” models). Still, using full capture history can allow estimate parameters (aren’t affected theses issues) can pretty interesting, like migratory behavior hunter participation.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Capture-Recapture-Overview.html","id":"estimating-abundance-or-demographic-parameters","dir":"Articles","previous_headings":"Key Terminology and Concepts","what":"Estimating abundance or demographic parameters","title":"Capture-Recapture: Overview","text":"Another way subdivide models whether goal estimate abundance demographic parameters (e.g., survival, recruitment, population growth rate, etc.). methods one key ways differentiate among methods. open closed models can well. Examples specific capture-recapture methods focus estimating abundance : Lincoln-Petersen (closed) Jolly-Seber (open) Closed Capture recapture (closed) Models/ approaches focus demographic parameters definition, open models, may account processes (immigration/emigration, survival, etc.). models primarily focus demographic processes : Cormack-Jolly-Seber (CJS) Jolly Seber Pradel Robust Design (combines closed open processes) Brownie/Seber (Dead encounters)","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Capture-Recapture-Overview.html","id":"unique-or-generic-markings","dir":"Articles","previous_headings":"Key Terminology and Concepts","what":"Unique or generic markings","title":"Capture-Recapture: Overview","text":"approaches require able uniquely identify individuals. Sometimes organism small marked unique tag, tags likely lost compared marking techniques (e.g scute marking turtles, fin clips), animal welfare logistical concerns prevent unique identification. However, generally flexibility can uniquely identify individuals. frequent reality , models use generic markings, many key assumptions met generally led researchers use individually identifiable markings. Still, cases generic markings useful (e.g. estimating waterfowl abundance direct band recoveries) can always “anonymize” uniquely marked organisms model requires generic markings. Lincoln (L) Schnabel (R) Lincoln-Petersen index related estimators (e.g. Chao, Schnabel), Jolly-Seber, types mark-resight models can use information generic markings, come relatively strict assumptions. approaches alternatives different formulations utilize unique markings permit deal real issues can cause bias (e.g. capture probability heterogeneity, non-closure). upshot , can deal violations assumptions easily unique markings. Options still exist generic marking, often, clever sampling design help ensure critical model assumptions met.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Capture-Recapture-Overview.html","id":"summary-and-extensions","dir":"Articles","previous_headings":"Key Terminology and Concepts","what":"Summary and Extensions","title":"Capture-Recapture: Overview","text":"may seem overwhelming number capture-recapture based methods literature clarifying discriminates among key assumptions can help sift . Understanding key concepts like population closure critical just using models correctly, important methods based capture recapture (e.g. N-mixture models), adapting models novel applications1 2","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Quick-Nimble-Demo.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Quick Nimble Demo","text":"just short demonstration executing MCMC estimation Bayesian model Nimble. largely also covered Nimble user manual. Specifically, walks executing Nimble “long way” several benefits. lets check initial values compiling avoid Nimble equivalent JAGS’s “Node inconsistent parents” errors lets customize samplers individual sets parameters.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Quick-Nimble-Demo.html","id":"nimble-vs--jags","dir":"Articles","previous_headings":"Introduction","what":"Nimble vs. JAGS","title":"Quick Nimble Demo","text":"Part makes Nimble attractive, compared JAGS, speed samples. speed comes computer-background-blackbox stuff , unless want deep dive programming, doesn’t matter (compiling,running C++). adds extra step getting model run. part makes Nimble efficient. part comes type samplers (algorithm used perform MCMC estimation) can specify. JAGS generally limits ability specify samplers model parameters. ’s sometimes better way, always. example, linear model many regression coefficients, may find JAGS samples slowly resulting chains mix poorly. probably default sampler (can’t change) slice sampler. works regression coefficient individually, trying sample one coefficient time. Slice sampling like can slow, often results chains independent samples/ lower autocorrelation. Nimble provides default samplers, lets adjust like. default sampler Nimble parameters (least continuous value ones, like regression coefficients) random-walk (Metropolis-Hastings) sampler. ’s quick, tends lead chains greater autocorrelation slice sampler, need run longer chains achieve similar effective sample sizes. However, Nimble lets specify block samplers. means , rather sampling regression coefficients individually, can sample together. tends slower, results less autocorrelation improved mixing.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/Quick-Nimble-Demo.html","id":"running-nimble","dir":"Articles","previous_headings":"","what":"Running Nimble","title":"Quick Nimble Demo","text":"’ll walk example using simulated data. One real nice thing Nimble uses BUGS programming language write model older models JAGS able used Nimble (even functions ). Simulating data Nimble model setup Configure MCMC setting monitors Build Compile MCMC Run model/get MCMC samples Next, write model BUGS ’ll use estimate parameters may already noticed, another thing can Nimble specify priors using standard deviation instead precision. don’t include sd= distribution, Nimble assume ’s precision, sure specify part prior distribution carefully. Next, prepare data. data, Nimble different JAGS makes distinction constants data. Data exclusively quantities come stochastic nodes. model , data supplying observed outcomes \\(y_i\\). covariates measured \\(x_{1i},x_{2i}\\) get supplied constants. variable want use indexing (n , loop) also constant. Now also set functions generate initial values MCMC chain. ’s better (always essential) initial values provided via function ensure chains don’t start value. ’s always critical provide initial values model mixes well, event mixing poor, starting chains different parameter values can help ensure MCMC searching widest range potential values possible (sampling full parameter space). initial value epsilon commented facilitate demonstration aspects Nimble later . Now bundle data, code, constants, initial values together ’ll call model object see message model fully initialized. usually means parameter can provide initial value , . case ’s epsilon, residual variance. first useful tool Nimble comes . even start trying sample, can get sense model run properly. message tells use model$initializeInfo(). tell nodes without initial values. ’s always fatal, ’s good practice try provide initial values. second command $calculate() tries calculate log probability density model initial values. hierarchical models, NA mean model probably won’t run, simpler models, can try. can also check particular nodes see giving trouble. know intializeInfo() NA didn’t provide value \\(epsilon\\) initial values. Sometimes though, still get NA even provided starting value. happens, ’ll need check individual nodes determine giving trouble make adjustments prior, initial value, . may require overwriting (re-run) nimbleCode /nimbleModel objects repeatedly. Just sake demonstration, ’ll fix issue providing intial value epsilon sort initial values create model object, configure MCMC sampler. Nimble automatically print nodes monitored assigned samplers execute configureMCMC call object, appear . case, conjugate sampler assigned regression coefficients prior likelihood (distribution observation) (normal). glm, wouldn’t conjugate likelihood possibly Poisson, logit, another distribution. random walk sampler, like epsilon. Conjugate samplers default might best generally perform well. Now though, want adjust samplers. , set second configuration use later , rather bouncing back forth. , set sampler residual variance slice sampler. set regression coefficients (b0, b1, b2) use automated factor slice sampler, AF_slice. form block sampler. also RW_block sampler, former often performs better. , blocking parameters just means parameters sampled together, rather individually. practice, likely applicable use regression coefficients, ’s something can play around . slice sampler, work parameters numerical, categorical binary. can see list possible samplers brief explanation Nimble converts model, data, MCMC algorithms C++ makes run faster ’s sampling. added cost though, need build MCMC sampler object compile model first. takes time just involves running 3 simple commands. 1.) buildMCMC using configuration object, leading MCMC object 2.) compileNimble using model object, leading compiled model 3.) compileNimble using compiled model MCMC object just created twice ensure everything need compare RW sampler slice sampler. Now can execute model obtain MCMC samples. set iterations, burning, chains, thinning. can use MCMCvis review summary statistics view traceplots. also use calls Sys.time() also track long sampler configuration runs. Look traceplots   better mixing slice sampling. Check summary statistics effective sample size parameter much larger using slice AF_slice sampler. time difference, slice sampler taking 0.55 seconds compared default samplers 0.28 seconds. ’s large difference simple model, ’s 2 times longer. scale models start taking longer run. isn’t clear answer samplers ultimately better. case running default samplers iterations quicker using block slice sampler. , default samplers may converge mix properly, regardless long run . just something end playing around testing.","code":"#simulate a multiple  regression  model   sim.dat<-function(N){  mu<-5  b.1<-1.5  b.2<--3    x.1<-runif(N,1,4) x.2<-runif(N,-2,2) err<-rnorm(N,0,0.5)  y<-mu+b.1*x.1+b.2*x.2+err  out.list<-list(y=y,x1=x.1,x2=x.2)  return(out.list) } library(nimble) #> nimble version 1.3.0 is loaded. #> For more information on NIMBLE and a User Manual, #> please visit https://R-nimble.org. #>  #> Note for advanced users who have written their own MCMC samplers: #>   As of version 0.13.0, NIMBLE's protocol for handling posterior #>   predictive nodes has changed in a way that could affect user-defined #>   samplers in some situations. Please see Section 15.5.1 of the User Manual. #>  #> Attaching package: 'nimble' #> The following object is masked from 'package:stats': #>  #>     simulate #> The following object is masked from 'package:base': #>  #>     declare  demo.code<-nimbleCode({   #priors   b0~dnorm(0,sd=100)   b1~dnorm(0,sd=100)   b2~dnorm(0,sd=100)   epsilon~dunif(0,10) # residual variance      #model    for (i in 1:n){      mu[i]<-b0+b1*x1[i]+b2*x2[i]      y[i]~dnorm(mu[i],sd=epsilon) }   }) set.seed(12345) demo.dat<-sim.dat(40)  nim.dat<-list(y=demo.dat$y)  nim.const<-list(n=length(demo.dat$x2),               x1=demo.dat$x1,                 x2=demo.dat$x2)  nim.inits<-function(){list(   #epsilon=runif(1,-5,0),   b0=rnorm(1,0,1),    b1=rnorm(1,0,1),   b2=rnorm(1,0,1)  )} demo.mod<-nimbleModel(code=demo.code,                       data=nim.dat,                       constants = nim.const,                       inits=nim.inits()) # remember the () for the initial values function! #> Defining model #> Building model #> Setting data and initial values #> Running calculate on model #>   [Note] Any error reports that follow may simply reflect missing values in model variables. #> Checking model sizes and dimensions #>   [Note] This model is not fully initialized. This is not an error. #>          To see which variables are not initialized, use model$initializeInfo(). #>          For more information on model initialization, see help(modelInitialization). demo.mod$initializeInfo() #>   [Note] Missing values (NAs) or non-finite values were found in model variables: epsilon. #>   [Note] This is not an error, but some or all variables may need to be initialized for certain algorithms to operate properly. #>   [Note] For more information on model initialization, see help(modelInitialization).  demo.mod$calculate() #> [1] NA  demo.mod$calculate('epsilon') #> [1] NA nim.inits<-function(){list(   epsilon=runif(1,0.001,5),   b0=rnorm(1,0,1),    b1=rnorm(1,0,1),   b2=rnorm(1,0,1)  )}    demo.mod<-nimbleModel(code=demo.code,                       data=nim.dat,                       constants = nim.const,                       inits=nim.inits()) #> Defining model #> Building model #> Setting data and initial values #> Running calculate on model #>   [Note] Any error reports that follow may simply reflect missing values in model variables. #> Checking model sizes and dimensions    demo.mod$initializeInfo() #>   [Note] All model variables are initialized.  demo.mod$calculate() #> [1] -211.9384  demo.mod$calculate('epsilon') #> [1] -2.302585 demo.config<-configureMCMC(demo.mod) #> ===== Monitors ===== #> thin = 1: b0, b1, b2, epsilon #> ===== Samplers ===== #> RW sampler (1) #>   - epsilon #> conjugate sampler (3) #>   - b0 #>   - b1 #>   - b2  demo.config #> ===== Monitors ===== #> thin = 1: b0, b1, b2, epsilon #> ===== Samplers ===== #> RW sampler (1) #>   - epsilon #> conjugate sampler (3) #>   - b0 #>   - b1 #>   - b2 demo.config.alt<-configureMCMC(demo.mod) #> ===== Monitors ===== #> thin = 1: b0, b1, b2, epsilon #> ===== Samplers ===== #> RW sampler (1) #>   - epsilon #> conjugate sampler (3) #>   - b0 #>   - b1 #>   - b2  demo.config.alt$replaceSamplers(target='epsilon',                                 type='slice') demo.config.alt$replaceSamplers(target=c('b0','b1','b2'),                                 type='AF_slice') demo.config #> ===== Monitors ===== #> thin = 1: b0, b1, b2, epsilon #> ===== Samplers ===== #> RW sampler (1) #>   - epsilon #> conjugate sampler (3) #>   - b0 #>   - b1 #>   - b2 demo.config.alt #> ===== Monitors ===== #> thin = 1: b0, b1, b2, epsilon #> ===== Samplers ===== #> slice sampler (1) #>   - epsilon #> AF_slice sampler (1) #>   - b0, b1, b2  # A third sampler configuration could be having each parameter be a slice sampler and sampled independently (targetByNode=TRUE). # This should be slower than RW or conjugate samplers, but have better mixing. It will be slower than the AF_slice sampler. #### not run  #  # demo.config.alt$replaceSamplers(target=c('b0','b1','b2','epsilon'),  #                             type='slice',  #                              targetByNode=TRUE) #----------- RW sampler on epsilon---------------------------   demo.build<-nimble::buildMCMC(demo.config) # build MCMC    demo.comp.mod<-nimble::compileNimble(demo.mod)   #first compile step   demo.comp.mcmc<-nimble::compileNimble(demo.build, project=demo.comp.mod) # second compile step    #----------- Slice sampler on epsilon   alt.build<-nimble::buildMCMC(demo.config.alt) # build MCMC    alt.comp.mod<-nimble::compileNimble(demo.mod)   #first compile step   alt.comp.mcmc<-nimble::compileNimble(alt.build, project=alt.comp.mod) # second compile step #-----------RW samplers RW.start<-Sys.time() demo.samples<-nimble::runMCMC(demo.comp.mcmc,                          niter=2000,                          nburnin = 1000,                          nchains = 3) RW.end<-Sys.time()  RW.time<-RW.end-RW.start   #------------slice sampler   slice.start<-Sys.time()  alt.samples<-nimble::runMCMC(alt.comp.mcmc,                          niter=2000,                          nburnin = 1000,                          nchains = 3)  slice.end<-Sys.time()  slice.time<-slice.end-slice.start MCMCvis::MCMCtrace(demo.samples,pdf=FALSE) MCMCvis::MCMCtrace(alt.samples,pdf=FALSE) MCMCvis::MCMCsummary(demo.samples)->RW.summary RW.summary #>               mean         sd       2.5%        50%      97.5% Rhat n.eff #> b0       5.3688498 0.31668017  4.7181530  5.3740203  5.9720205 1.02   167 #> b1       1.3864821 0.12001356  1.1585818  1.3836834  1.6237416 1.02   165 #> b2      -2.9880842 0.08505345 -3.1549402 -2.9864336 -2.8260734 1.00  1308 #> epsilon  0.6318047 0.07263246  0.5129054  0.6259802  0.7989728 1.00   798   MCMCvis::MCMCsummary(alt.samples)->slice.summary slice.summary #>               mean         sd       2.5%       50%      97.5% Rhat n.eff #> b0       5.4075780 0.32059372  4.7680382  5.410258  6.0419999 1.00  3240 #> b1       1.3725860 0.12049228  1.1322053  1.370473  1.6079879 1.00  3000 #> b2      -2.9830657 0.08727785 -3.1532691 -2.985470 -2.8062216 1.01  2718 #> epsilon  0.6368656 0.07732851  0.5076331  0.628372  0.8070473 1.00  2305"},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Time To Event and Known-fate (Survival) Analyses","text":"Time--event analysis can broad encompasses variety methods. us probably encounter type data survival analysis tend use examples application, much flexible . ’s core time event data data : known well-defined “origin” time, start time sample (individual) risk event interest happening. quantity interest time event occurs (long X happens) covariates affect time. event must categorical, either Bernoulli variable (live/dead) multinomial (live, dead cause , dead cause B…) must able determine event happened, event happened (.e. known-fate). Ecologists typically encounter survival analysis event interest death, can almost anything. ecology, ’s used model resources selection (First-passage-time1), epidemiology disease dynamics2, breeding ecology. 3 ’s also core space--event model estimating animal density.4 Importantly, event interest doesn’t just need something can occur (like death), TTE framework can accommodate recurring events. Still, ’ll focus application survival analysis since ’s common can use methods folks might familiar provide context. said, within survival analysis, might often feel like ’re talking circles use one method another can functionally accomplish thing (generally GLM analog things). links TTE models (survival analysis) GLMs using logit probit link , TTE analysis models time event occurring, whereas GLMs model “probability ” event occurring. Time explicit TTE, whereas analyses, might include covariate. ’d argue hazard model confusing use, unless really natural logical “origin” time.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"key-concepts","dir":"Articles","previous_headings":"","what":"Key Concepts","title":"Time To Event and Known-fate (Survival) Analyses","text":"key concepts important understanding TTE analyses relates analyses.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"censoring-and-truncation","dir":"Articles","previous_headings":"Key Concepts","what":"Censoring and Truncation","title":"Time To Event and Known-fate (Survival) Analyses","text":"Censoring truncation common survival analyses ’ve dealt whether ’ve used TTE analyses GLM (e.g. logistic exposure).","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"interval-censoring","dir":"Articles","previous_headings":"Key Concepts > Censoring and Truncation","what":"Interval Censoring","title":"Time To Event and Known-fate (Survival) Analyses","text":"common method censoring interval censoring. don’t know exact event time, just occurred sometime interval observation. unless checking nests radio-marked individuals every day, interval censored data. fitting GLM logit link, probably dealt interval censoring using logistic exposure link another technique (like cloglog link + offset). familiar Program MARK used known-fate module , concept interval censoring might seem confusing (least) , lose track individual, code interval ‘00’ indicate individual missing, thus, censor time period. refer interval censoring, ’s “interval censoring” mentioned texts regarding TTE survival analyses. ’s accurately called interval truncation DeCesare et.al (20155) give good explanation. ’s artifact Program MARK using model permit interval censoring. even tell use nest-survival module interval censoring. Compare TTE analysis almost analysis ’ll use interval censoring allowed. , need drop observations. animal observed detected dead later , interval censoring handles uncertainty mortality occurred. observe alive later know survived. never encounter dead alive , ’s just right censoring, relying assumption censoring informative. also requires probability detecting individual doesn’t depend fate-probability finding individual went missing , regardless finding dead alive.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"right-censoring-and-informative-censoring","dir":"Articles","previous_headings":"Key Concepts > Censoring and Truncation","what":"Right Censoring and informative censoring","title":"Time To Event and Known-fate (Survival) Analyses","text":"lose track individual never find (alive dead) haven’t died yet study ends, right-censoring. latter case, ’s called fixed censoring, whereas former, ’s assumed random. , dying doesn’t cause loose track individual. censoring informative (radio transmitters fail destroyed cars/people/predators) survival estimate biased. true regardless analyze data TTE framework logistic regression framework. random (least nearly) doesn’t matter. lose track lot individuals without knowing fate, ’s recommended don’t use approach relies accurately knowing .","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"left-censoring-and-truncation","dir":"Articles","previous_headings":"Key Concepts > Censoring and Truncation","what":"Left Censoring and Truncation","title":"Time To Event and Known-fate (Survival) Analyses","text":"Left censoring doesn’t make sense survival analysis, means event interest occurred individual became part study. nonsense ’s death. Left censoring undoubtedly occurs data sets, nothing can typically , statistically speaking, account . typically data sets truncation. Left-truncation applies individuals observed origin time, experience time wildlife data; individuals survive long enough us capture . Truncation can accommodated (TTE analysis GLM) specify time capture logit model entry time (relative origin). gets hard wrap head around. whole issue left trunction censoring really makes sense really “origin time”, otherwise often just get taught ways avoid deal . Hazard models accommodate left truncation considering interval individual first observed. GLM, generally treat “heterogeneity” deal including appropriate covariate, age--capture, maybe including age time-varying covariate. Clearly doesn’t correct fact might observing individuals age/stage classes. really isn’t statistical approach “fix” -’s sampling issue. ’s generally talk survival estimates specific age-classes stages.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"methods","dir":"Articles","previous_headings":"","what":"Methods","title":"Time To Event and Known-fate (Survival) Analyses","text":"quick reminder, methods used derive “survival” estimate, even though modeling survival directly (e.g. TTE).","code":""},{"path":[]},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"glm-approaches--logit-link","dir":"Articles","previous_headings":"Methods > Modeling survival vs Hazards","what":"GLM approaches -Logit link","title":"Time To Event and Known-fate (Survival) Analyses","text":"talking survival time-event analysis ’s helpful start talking using GLM estimate survival (mortality) ’s probably folks familiar . first big difference using GLM, ’re modeling probability event occurring interval, long occurs. generally use approach (opposed TTE method), really don’t ability define origin time, typically individuals entering leaving study varying time points / can’t accurately age individuals. approach, data probably table , minimum, rows equal number individuals x number intervals column indicating individual survived died interval (e.g. 1/0). intervals regular equal length can use generic, built-methods estimating survival (e.g. logit link). interested modeling survival function time (age, day year), typically deal using interval-specific covariate. Left truncation can dealt way, interval truncation can handled omitting intervals unable verify status individual (might necessary), right censoring handled implicitly individual longer appearing records. checks irregular intervals, experience trivial amount interval truncation, better accounting variable length time interval using logistic exposure method.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"kaplan-meier","dir":"Articles","previous_headings":"Methods > Modeling survival vs Hazards","what":"Kaplan-Meier","title":"Time To Event and Known-fate (Survival) Analyses","text":"Kaplan-Meier called non-parametric method quantities describe (cumulative survival, period survival) obtained simple arithmetic : \\[ S_t=\\frac{N_{alive\\ \\ t}}{N_{alive\\ t-1}}\\] ’re using linear model estimate parameters. use discrete covariates, logit model described equivalent Kaplan-Meier estimate. However, logit model gives flexibility, namely types covariates can include. Furthermore, Kaplan-Meier estimates assume interval censoring. ’s generally best bet don’t many individuals data set. Plots Kaplan-Meier estimates typically show cumulative survival probability function time stair-step pattern. Note implying survival probability varies time. Kaplan-Meier estimates survival time step, visualized proportion individuals remaining time T, equivalent \\(S^T\\), \\(S\\) 1-unit survival probability.  can see GLM- constant survival estimates cumulative survival probability lower observed data, end, KM estimate shows. Nevertheless, produce average daily survival rate. GLM allowed time-varying survival better approximation KM curve, also may just overfitting, data plot simulated using constant survival probability.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"log-log-link","dir":"Articles","previous_headings":"Methods > Modeling survival vs Hazards","what":"Log-Log link","title":"Time To Event and Known-fate (Survival) Analyses","text":"alternative using logit-link regression use log-log link : \\[ S=exp(-exp(X\\beta))\\\\ \\ \\ inverse\\\\ X\\beta=log(-log(S))\\] \\(S\\) survival probability \\(X\\beta\\) linear predictor. approach gives interpretation (hazard ratios) \\(\\beta_k\\) regression coefficients proportional hazard models. Often preferable using logit link hazards ratios (relative difference mortality) don’t change whether model talking weekly, monthly, annual survival. Odds ratios differ, may just confusing worse, misleading. R, use complementary log-log link, equivalent replaceing \\(S\\) \\(M\\), \\(X\\beta\\) predicts. R, still code data 1=alive, 0=dead, essentially modeling 0’s using data structure cloglog link. Interval censoring can accommodated using logit-link using logistic-exposure link, including offset number exposure days interval using cloglog link.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"modeling-time-to-event","dir":"Articles","previous_headings":"Methods","what":"Modeling time-to-event","title":"Time To Event and Known-fate (Survival) Analyses","text":"name implies, ’re estimating probability event occurs specific interval (like GLM), instead amount time takes event happen. gets discussed modeling process continuous time (TTE) vs. discrete time. Thus, instead calculating probability event occurs interval, TTE estimating number events/deaths “immortal individual” might experience time frame. Coming background modeling nest survival discrete time, big mental flip wrapping head around stopping thinking analyses like graphs DSR instead, thinking graphs show cumulative survival. ’s curve, called survivor function TTE estimating. derivative curve time point \\(h(t)\\) hazard function. math develop survivor function fundamentally different using something like logstic (exposure) regression. using logit link, can develop equivalent information computing product logit-transformed linear predictors \\[Discrete\\ time ,\\ logit\\ link\\\\ S_{n|1}=\\prod_{t=1}^{N}S_t=S_1\\cdot S_2\\cdot S_3...S_n\\\\ S_t=\\frac{exp(X\\beta)}{1+exp(X\\beta)}\\] looks like used proportional hazards model \\[TTE\\ \\ Continuous\\ Time-Proportional Hazards\\\\ S_{1|n}=exp(-\\sum_{t=1}^{N}{exp(X\\beta)})\\] doesn’t look like much different, makes math things easier. Particularly, mentioned , comparing effect covariate outcome different unit initially measured \\(t\\) . also becomes easier model wider nonlinear effects time age survival typically available GLM (e.g. linear, quadratic, cubic…). figure right demonstrates . comes TTE workshop TWS meeting Reno 2019, though data long-term deer survival study northern MN.  textbook definition hazard instantaneous rate mortality. hazard high, mortality high vice versa. TTE focuses estimating timing events regardless actual procedure. simplest form TTE linear model \\[h(t) = \\lambda_0 \\cdot \\lambda_t\\] hazard time \\(t\\), product baseline hazard\\(\\lambda_0\\), covariates \\(\\lambda_t\\) can modeled using log link. \\[\\lambda_t=exp(\\beta_1X_1+\\beta_2X_2...\\beta_kX_k)\\] particular formulation proportional hazards (PH) model. Proportional hazards models can semi-parametric, don’t try specify distribution baseline hazard, fully parametric . distribution used specify baseline hazard can dramatic effect shape hazard curve whether model still proportional hazards model, now becomes accelerated-failure-time (AFT) model, something else.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"semi-parametric","dir":"Articles","previous_headings":"Methods > Modeling time-to-event","what":"Semi parametric","title":"Time To Event and Known-fate (Survival) Analyses","text":"semi-parametric hazard model synonymous Cox’s proportional hazard. , however, version proportional hazard model. Cox gets name attached came slick way estimate regression coefficients \\(\\lambda_t\\) without ever estimate \\(\\lambda_0\\), using partial likelihood. People don’t really like trying specify baseline ’s often preferable use approach. However, true Cox’s proportional hazard model doesn’t work interval censoring, applications probably use parametric hazards model. example, interval censoring want use proportional hazard model, actually fit parametric hazard model, using exponential baseline hazard. Important note, like cloglog link, modeling mortality, negative coefficients (log-hazard ratios) mean negative association mortality. ’s also worth noting without specifying kind distribution baseline hazard, can get survivor curve estimating baseline non-parametric methods. hard, using parametric, exponential model equivalent. ’s straightforward just !","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"parametric-hazards","dir":"Articles","previous_headings":"Methods > Modeling time-to-event","what":"Parametric Hazards","title":"Time To Event and Known-fate (Survival) Analyses","text":"Parametric hazards specify distribution baseline hazard. Proportional hazards (PH), proportional odds (PO), accelerated-failure-time (AFT) models types parametric TTE models. difference models : link used model hazard whether hazard changes time. Another way see connections re-write time event model linear function times event, hazard: \\[log(T)=\\beta_0+\\beta_1X_1+\\beta_2X_2...+\\sigma\\epsilon\\] random disturbance term \\(\\epsilon\\) follows distribution. software routines don’t estimate variance \\(\\epsilon\\) hold constant estimate \\(\\sigma\\) scaling parameter instead. described Paul Allison SAS survival book, depending distribution assume \\(\\epsilon\\), corresponding hazard function: \\(\\epsilon\\) extreme-value (exponential distribution) \\(\\sigma\\) fixed one T exponentially distributed gives hazard function: \\[log(h(t))=\\beta_0^*+\\beta_1^*X_1...\\beta_k^*X_k\\] functionally hazard model used Cox’s PH now intercept. exponential model, constant hazard nothing specifically models hazard function time. term function time, AFT model. \\(\\epsilon\\) extreme-value (exponential distribution) \\(\\sigma\\) isn’t fixed 1, T follows Weibull distribution , per SAS survival book: \\[log(h(t))=\\alpha\\log(t)+\\beta_0^*+\\beta_1^*X_1...\\beta_k^*X_k\\] now AFT, depending parameter value \\(\\alpha\\)(see pp. 20-21 SAS survival book). case, \\(\\alpha\\) controls hazard varies across time, ensuring event times T follow Weibull distribution. \\(\\alpha\\) just 0, hazard constant time ’re back exponential distribution. Fun fact, Weibull AFT PH model, ’s form can . ’s , equation , think \\(\\alpha\\log(t)\\) baseline hazard Another way write Weibull hazard can found handout 9 : \\[h(t)=\\lambda p t^{p-1}\\\\ log(h(t))=log(\\lambda)+log(p)+(p-1)log(t)\\\\ log(h(t))=\\alpha\\ log(t) +\\beta_0^*+\\beta_1^*X_1...\\beta_k^*X_k\\] \\(p\\) \\(\\frac{1}{\\sigma}\\) \\(log(T)\\) equation . looks like difference two resources \\(\\alpha \\sim p-1\\) value log(p) seems end \\(\\beta_0\\)? forms include Log logistic \\[h(t)=\\frac{\\lambda p t^{p-1}}{1+\\lambda t^p}\\] gives proportional odds model. AFT models Typically, however, fitting AFT model ’re writing linear model hazard, linear model \\(log(T)\\) coefficients expressed effect survival time, hazard.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"what-makes-a-model-an-aft-andor-ph","dir":"Articles","previous_headings":"Methods > Modeling time-to-event","what":"What makes a model an AFT and/or PH?","title":"Time To Event and Known-fate (Survival) Analyses","text":"graphs help visually demonstrate , lecture notes found . PH proportional hazards, constant hazard rate. short proportional hazards model assumes hazard ratio doesn’t change time changes. graph hazard two groups time parallel, peak hazard occurring time. AFT, hazard peaks earlier later one group. ’s difference /vs. left/right. meant books/slides say AFT “stretches” time AFT. big difference fit AFT model, code (existing package) put linear model survival time, hazard. anything exponential distribution gets really messy looking hazard scale. can still derive , ’s straight forward write linear model \\(T\\), \\(h(t)\\) AFT models also PH models (e.g. Weibull), aren’t. don’t need . whole value AFT models meet proportional hazards assumption (.e. hazard ratio changes time). Still AFT, need specify baseline distribution T. typically won’t use AFT left truncation-? need data help fit curve/ determine appropriate shape baseline hazard. want risky specify one anyway, hope ’re right, can “predict” outside sample survival times individuals never observed. ’s hard know distribution use baseline hazard (.e. hazard vary time) ’s usually easier turn exponential PH model (technically AFT, constant baseline hazard) throw tricks account temporal variation non-constant hazard ratios (see slides Reno). approach works probably end time, gets complicated (e.g. time-varying coefficients, piecewise exponential model), think feels little “Dr. Moreau” (clearly, Brando’s best film ever)","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"example-nest-survival","dir":"Articles","previous_headings":"","what":"Example: nest survival","title":"Time To Event and Known-fate (Survival) Analyses","text":"shows time-varying survival might incorporated hazards model GLM framework. approach assumes know age nest discovery, nests discovered first day. also assumes check active nests 25th day, day nesting attempt complete. , TTE model doesn’t make sense unless logical origin time.","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"constant-survival","dir":"Articles","previous_headings":"Example: nest survival","what":"Constant survival","title":"Time To Event and Known-fate (Survival) Analyses","text":"Simulate data mean DSR 0.985 nesting period 25 days TTE data look like one realization simulated data. Left date nest discovered, right last day monitored, censor refers nest survived interval. Note nests failed point two rows, one representing interval time nest known alive, another interval nest failed. data formatted use nimble model specified . Table 1. Time event data format nest.id left right censor 1 2 13 1 1 13 14 0 2 1 25 1 3 1 25 1 4 1 25 1 5 8 25 1 6 1 25 1 7 3 16 1 7 16 18 0 8 1 25 1 9 1 16 1 9 16 17 0 10 1 25 1 11 1 3 0 12 3 25 1 Survive” package R otherwise fit Cox proportional hazards model handle interval censoring can’t use . ways specify interval censoring right censoring using survreg function “survival” package fits parametric models, specify exponential distribution, ’s equivalent proportional hazards model. , however, assumes monitoring individuals time origin (.e., left truncation)  GLM Data looks like Table 2. subset GLM survival data nest.id day exposure obs.ld 1 6 4 1 1 7 1 1 1 8 1 1 1 9 1 1 1 10 1 1 1 11 1 1 1 13 2 1 1 14 1 0 2 2 1 1 2 3 1 1 2 4 1 1 2 5 1 1 2 7 2 1 2 9 2 1 2 10 1 1","code":""},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"analysis-using-a-tte-hazards-model","dir":"Articles","previous_headings":"Example: nest survival > Constant survival","what":"Analysis using a TTE hazards model","title":"Time To Event and Known-fate (Survival) Analyses","text":"model parametric model exponential distribution use simple exponential function, baseline hazard constant generally produce hazard ratios Cox PH model doesn’t assume baseline hazard.  gamma parameter needs transformed back survival. , just exponentiate value linear scale (case, just gamma, include terms). isn’t surival probability, ’s mortality probability. true transformation : \\[S=1-exp(\\gamma_0)\\\\ \\\\ S=1-exp(\\gamma_0+\\gamma_1X_1+\\gamma_2X_2...)\\] covariates, \\(\\gamma_k\\) log hazard ratios (equivalent \\(\\beta\\) GLM). means estimated daily survival TTE model 0.984.","code":"library(nimble)  constant.model<-nimble::nimbleCode({  gamma0~dflat()     for (j in 1:records) {     for (k in left[j]:(right[j]-1)) {       UCH[j,k] <- exp(gamma0)     }      S[j] <- exp(-sum(UCH[j,left[j]:(right[j]-1)]))     censored[j] ~ dbern(S[j])   }    for (i in 1:25) { #compute the survival function S0     UCH0[i]<-exp(gamma0)     CH0[i]<-sum(UCH0[1:i]) #sum the unit cumulative hazard to get the cumulative                  #hazard (i.e prob of surviving day 1 to i, or S^2,S^3..S^i     S0[i]<-exp(-CH0[i]) # transform to survival estimate to time i   } })    #define data and constants  nim.const<-list(records=nrow(tte.dat),                 left=tte.dat$left,                 right=tte.dat$right) nim.dat<-list(censored=tte.dat$censor)   #inits  nim.init<-function(){list(   gamma0=runif(1,-3,3) ) }   constant.nimmod<-nimble::nimbleModel(code=constant.model,                                constants=nim.const,                                data=nim.dat,                                inits=nim.init())  constant.nimmod$calculate()  # Check to make sure initial values are good.                               #Should be a real number   constant.config<-nimble::configureMCMC(constant.nimmod)  # now is when you can specify different samplers and monitors constant.config$addMonitors(\"CH0\",\"UCH0\",\"S0\")   constant.build<-nimble::buildMCMC(constant.config)   #compile things  constant.comp<-nimble::compileNimble(constant.nimmod)  constant.mcmc<-nimble::compileNimble(constant.build, project=constant.comp)   samples.constant<-nimble::runMCMC(constant.mcmc,                              niter=5000,                              nburnin = 1000,                              nchains = 3) ##             mean        sd     2.5%       50%     97.5% Rhat n.eff ## gamma0 -4.153043 0.2928866 -4.76659 -4.136809 -3.627007    1  2676"},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"analysis-using-logistic-exposure","dir":"Articles","previous_headings":"Example: nest survival > Constant survival","what":"Analysis using logistic exposure","title":"Time To Event and Known-fate (Survival) Analyses","text":"link function needs GlobalEnvironment. Alternatively, ’s included function MDChelp package using ‘MDChelp::logexp()’","code":"#####   Create the link function # Link function comes from Ben Bolker's page: https://rpubs.com/bbolker/logregexp  logexp <- function(exposure = 1) {   ## hack to help with visualization, post-prediction etc etc   get_exposure <- function() {     if (exists(\"..exposure\", env=.GlobalEnv))       return(get(\"..exposure\", envir=.GlobalEnv))     exposure   }   linkfun <- function(mu) qlogis(mu^(1/get_exposure()))   ## FIXME: is there some trick we can play here to allow   ##   evaluation in the context of the 'data' argument?   linkinv <- function(eta) plogis(eta)^get_exposure()   logit_mu_eta <- function(eta) {     ifelse(abs(eta)>30,.Machine$double.eps,            exp(eta)/(1+exp(eta))^2)   }   mu.eta <- function(eta) {     get_exposure() * plogis(eta)^(get_exposure()-1) *       logit_mu_eta(eta)   }   valideta <- function(eta) TRUE   link <- paste(\"logexp(\", deparse(substitute(exposure)), \")\",                 sep=\"\")   structure(list(linkfun = linkfun, linkinv = linkinv,                  mu.eta = mu.eta, valideta = valideta,                  name = link),             class = \"link-glm\") } ###########   mod.logexp<-glm(obs.ld~1,family=binomial(link=MDChelp::logexp(glm.dat$exposure)),                 data=glm.dat)  broom::tidy(mod.logexp)->parms.logit  parms.logit ## # A tibble: 1 × 5 ##   term        estimate std.error statistic  p.value ##   <chr>          <dbl>     <dbl>     <dbl>    <dbl> ## 1 (Intercept)     4.02     0.280      14.4 1.00e-46 #DSR plogis(parms.logit%>%   pull(estimate))->dsr.logit  dsr.logit ## [1] 0.9824191"},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"analysis-using-the-cloglog-link-and-exposure-time-as-an-offset-","dir":"Articles","previous_headings":"Example: nest survival > Constant survival","what":"Analysis using the cloglog link and exposure time as an offset.","title":"Time To Event and Known-fate (Survival) Analyses","text":"back-transforming, need mean mean “exposure” variable. Now compare among methods See similar one another less 0.005 different data-generating value 0.985? See small effect cumulative survival probability \\((DSR^{24})\\) (24 survival periods survive 25 days), just sampling error. mindful, demure. Table 3. Survival estimate comparison among methods Parameter Hazard Logistic exposure Cloglog + offset() \"Truth\" DSR 0.984 0.982 0.984 0.985 Cumulative survival(25d) 0.684 0.653 0.687 0.696","code":"mod.clog<-glm(obs.ld~1+offset(log(exposure)),               family=binomial(link='cloglog'),               data=glm.dat)   broom::tidy(mod.clog)->parms.clog  parms.clog ## # A tibble: 1 × 5 ##   term        estimate std.error statistic  p.value ##   <chr>          <dbl>     <dbl>     <dbl>    <dbl> ## 1 (Intercept)    0.975    0.0745      13.1 4.36e-39 #calculate mean exposure mu.exp<-glm.dat%>%pull(exposure)%>%mean()  data.frame(parms.clog[1,2]+log(mu.exp))%>%   rename(lp=1)%>%   mutate(dsr=1-exp(-exp(lp)))%>%   pull(dsr)->dsr.clog  round(dsr.clog,digits=3) ## [1] 0.984"},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"survival-as-a-function-of-time","dir":"Articles","previous_headings":"Example: nest survival","what":"Survival as a function of time","title":"Time To Event and Known-fate (Survival) Analyses","text":"mentioned biggest advantage TTE models ease one can model time-varying survival compute cumulative survival probability (parametric models ). time ’ll simulate data increase survival nest age increases. survival probability first interval 0.93 regression coefficent 0.3.","code":"#simulate data library(tidyverse) set.seed(1234)  max.age=25 N=40 age.seq<-seq(0,23,1) mu.s=qlogis(0.93) beta.age=0.3 S<-plogis(mu.s+beta.age*age.seq) cumprod(S)   true.mat<-matrix(NA,nrow=N,ncol=max.age)    true.mat[,1]<-1    for(i in 1:N){    for(t in 2:max.age){            true.mat[i,t]<-true.mat[i,t-1]*rbinom(1,1,S[t-1])    }  }   p.mat<-matrix(rbinom(N*max.age,1,0.35),nrow=N,ncol=max.age) # matrix if nest is observed  #p.mat[,1]<-1 # ensure all individuals observed at time=1  p.mat[,max.age]<-1 #all individuals observed by the end (no right censoring)  true.mat[true.mat==0]<- -1 #assign -1 to dead individuals for later processing ease  obs.mat<-true.mat*p.mat   obs.mat%>%   as.data.frame()%>%   rownames_to_column(var=\"nest.id\")%>%     pivot_longer(cols=2:(max.age+1),                names_to='time',                values_to='obs')%>%   mutate(time=as.numeric(str_remove(time,'V')))%>%   filter(obs!=0)%>%   group_by(nest.id)%>%   filter(!all(obs<1))%>%   mutate(exposure=time-lag(time),          flag=obs*lag(obs))%>%   ungroup()%>%   filter((obs==1 |(obs==-1 & flag==-1)))%>%   select(-flag)%>%   mutate(y=ifelse(obs==-1,0,1))->eh.long    eh.long%>%   select(nest.id,time,y)%>%   group_by(nest.id)%>%   filter(y==0|          time %in% max(time[y==1])|           time %in% min(time[y==1]))%>%   mutate(right=lead(time),          censor=lead(y))%>%   filter(!(is.na(right)))%>%   select(nest.id,left=time,right,censor)->tte.time.dat   eh.long%>%   ungroup()%>%   filter(!is.na(exposure))->glm.time.dat"},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"time-varying-hazards-model","dir":"Articles","previous_headings":"Example: nest survival > Survival as a function of time","what":"Time-varying hazards model","title":"Time To Event and Known-fate (Survival) Analyses","text":"fit time-varying hazards model, fit exponential model , include term account time. method just includes nest age covariate, alternative use (improper) conditional autoregressive model. iCAR model doesn’t try specify kind functional relationship (like linear model), can challenging fit.","code":"library(nimble)  time.model<-nimble::nimbleCode({  gamma0~dflat()   beta~dflat()    for(i in 1:24){   age[i]<-beta*nest.age[i] }   for (j in 1:records) {     for (k in left[j]:(right[j]-1)) {              UCH[j,k] <- exp(gamma0+age[k])     }      S[j] <- exp(-sum(UCH[j,left[j]:(right[j]-1)]))     censored[j] ~ dbern(S[j])   }    for (i in 1:24) { #compute the survival function S0     UCH0[i]<-exp(gamma0+age[i])     CH0[i]<-sum(UCH0[1:i]) #sum the unit cumulative hazard to get the cumulative                  #hazard (i.e prob of surviving day 1 to i, or S^2,S^3..S^i     S0[i]<-exp(-CH0[i]) # transform to survival estimate to time i   } })    #define data and constants  nim.const<-list(records=nrow(tte.time.dat),                 left=tte.time.dat$left,                 right=tte.time.dat$right,                 nest.age=seq(0,23,1)) nim.dat<-list(censored=tte.time.dat$censor)   #inits  nim.init<-function(){list(   gamma0=runif(1,-10,-1),   beta=runif(1,-10,-1)   #S=runif(nrow(tte.time.dat),0.5,0.9),   #UCH=matrix(runif(nrow(tte.time.dat)*25,5,10),nrow=nim.const$records,ncol=25) ) }   time.nimmod<-nimble::nimbleModel(code=time.model,                                constants=nim.const,                                data=nim.dat,                                inits=nim.init())  time.nimmod$calculate()  # Check to make sure initial values are good.                               #Should be a real number time.nimmod$initializeInfo()  time.config<-nimble::configureMCMC(time.nimmod)  # now is when you can specify different samplers and monitors time.config$addMonitors(\"CH0\",\"UCH0\",\"S0\")  time.config$replaceSamplers(target = c('gamma0','beta'),                        type= 'AF_slice')  time.build<-nimble::buildMCMC(time.config)   #compile things  time.comp<-nimble::compileNimble(time.nimmod)  time.mcmc<-nimble::compileNimble(time.build, project=time.comp)   samples.time<-nimble::runMCMC(time.mcmc,                              niter=10000,                              nburnin = 3000,                              nchains = 3) ##        parm          med   dsr.med cumsurv.med ## 1   UCH0[1] 0.0450048247 0.9549952   0.9549952 ## 2   UCH0[2] 0.0347043221 0.9652957   0.9202909 ## 3   UCH0[3] 0.0266844750 0.9733155   0.8936064 ## 23 UCH0[23] 0.0001762815 0.9998237   0.8031316 ## 24 UCH0[24] 0.0001372484 0.9998628   0.8029943 ## 25 UCH0[25] 0.0001070775 0.9998929   0.8028873"},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"analysis-using-logistic-exposure-1","dir":"Articles","previous_headings":"Example: nest survival > Survival as a function of time","what":"Analysis using logistic exposure","title":"Time To Event and Known-fate (Survival) Analyses","text":"","code":"###########    mod.logexp<-glm(y~time,                 family=binomial(                   link=MDChelp::logexp(glm.time.dat$exposure)),                 data=glm.time.dat)  broom::tidy(mod.logexp)->parms.logit    # parms.logit   #DSR parms.logit%>%   pull(estimate)->dsr.logit # round(dsr.logit,digits=3)  age=seq(1,24,1)  dsr.logexp<-plogis(dsr.logit[1]+dsr.logit[2]*age)   # dsr.logexp"},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"analysis-using-the-cloglog-link-and-exposure-time-as-an-offset--1","dir":"Articles","previous_headings":"Example: nest survival > Survival as a function of time","what":"Analysis using the cloglog link and exposure time as an offset.","title":"Time To Event and Known-fate (Survival) Analyses","text":"turns isn’t advisable exposure isn’t roughly . gives biased estimates. think part issue , arises shape cloglog distribution near tails probability. want hazards, just run hazards model.","code":"glm.time.dat$exposure    mod.clog<-glmmTMB::glmmTMB(y~time+exposure,               family=binomial(link='cloglog'),               data=glm.time.dat)  glmmTMB::fixef(mod.clog)$cond->parms.clog  parms.clog  age=seq(1,24,1) median(glm.time.dat$exposure) #offset not included because log(1) (1 exposure day) = 0  lp= parms.clog[1]+parms.clog[2]*age # linear predicator  dsr.clog=1-exp(-exp(lp))  dsr.clog  prod(dsr.clog)"},{"path":"https://tlyons253.github.io/MDChelp/articles/TTE-Advanced.html","id":"state-space-or-hmm-formulation","dir":"Articles","previous_headings":"Example: nest survival > Survival as a function of time","what":"State-space or HMM formulation","title":"Time To Event and Known-fate (Survival) Analyses","text":"last step going equivalent Program MARK . ’s Bayesian don’t reformat data anything like haven’t figured now, ’m kind lazy.  Table 4. Daily survival estimate comparison among methods, (time-varying survival). Model Age 1 Age 2 Age 3 Age 4 Age 5 Age 6 Age 7 Age 8 Age 9 Age 10 Age 11 Age 12 Age 13 Age 14 Age 15 Age 16 Age 17 Age 18 Age 19 Age 20 Age 21 Age 22 Age 23 Age 24 Hazard 0.9550 0.9203 0.8936 0.8732 0.8575 0.8454 0.8361 0.8287 0.8230 0.8185 0.8150 0.8122 0.8101 0.8084 0.8071 0.8061 0.8053 0.8047 0.8042 0.8038 0.8035 0.8033 0.8031 0.8030 Logistic exposure 0.9365 0.9474 0.9565 0.9641 0.9705 0.9757 0.9800 0.9836 0.9865 0.9889 0.9909 0.9926 0.9939 0.9950 0.9959 0.9966 0.9972 0.9977 0.9982 0.9985 0.9988 0.9990 0.9992 0.9993 State Space 0.9622 0.9684 0.9737 0.9781 0.9817 0.9848 0.9874 0.9895 0.9913 0.9928 0.9940 0.9950 0.9959 0.9966 0.9971 0.9976 0.9980 0.9984 0.9986 0.9989 0.9991 0.9992 0.9994 0.9995 \"Truth\" 0.9300 0.9472 0.9603 0.9703 0.9778 0.9835 0.9877 0.9909 0.9932 0.9950 0.9963 0.9972 0.9979 0.9985 0.9989 0.9992 0.9994 0.9995 0.9997 0.9997 0.9998 0.9999 0.9999 0.9999 Table 5. Cumulative survival estimate comparison among methods; (time-varying survival). Parameter Hazard Logistic exposure State Space \"Truth\" 25d survival 0.803 0.695 0.798 0.753 Now fun part. differences daily survival rates among different methods, compared “Truth” (Table 4.) doesn’t seem bad, ’s likely due part, just random sampling error (just one realization truth). point rate change different among three different methods, due underlying distributions (exponential-hazard, modified logistic-logistic exposure, logistic-state space). leads non-trivial differences estimates cumulative survival probability (Table 5.). feature, flaw. Even going back Shaffer 20046, daily survival estimates produced logistic exposure method Program MARK different lead substantially different cumulative survival probabilities. example provided, discrepancy among methods probably also function baseline survival probability quickly increased. might bad , case, DSR never got close 1, increased slowly, started lower, cumulative survival period shorter. ? expected model time-varying survival, probably use hazard state-space formulation. might seem ’s seem agreement, ’s . logistic-exposure framework, covariates assumed constant across exposure period. clearly realistic talking nest age, time-varying covariates can measure even know nest fate corresponding days (daily temperature, daily rainfall, etc.) methods however can, think probably makes little better, across wide range conditions/ parameter values. said, models going wrong, useful whatever works best situation.","code":"##  [1] 0.9621578 0.9684107 0.9736588 0.9780547 0.9817307 0.9848006 0.9873613 ##  [8] 0.9894951 0.9912719 0.9927504 0.9939799 0.9950020 0.9958512 0.9965567 ## [15] 0.9971425 0.9976289 0.9980327 0.9983678 0.9986459 0.9988767 0.9990682 ## [22] 0.9992271 0.9993589 0.9994682 ## [1] 0.7981269"},{"path":"https://tlyons253.github.io/MDChelp/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Tim Lyons. Author, maintainer.","code":""},{"path":"https://tlyons253.github.io/MDChelp/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lyons T (2025). MDChelp: Help Environmental Statistical Methods. R package version 0.0.0.9000, https://github.com/tlyons253/MDChelp.","code":"@Manual{,   title = {MDChelp: Help With Environmental Statistical Methods},   author = {Tim Lyons},   year = {2025},   note = {R package version 0.0.0.9000},   url = {https://github.com/tlyons253/MDChelp}, }"},{"path":"https://tlyons253.github.io/MDChelp/index.html","id":"mdchelp","dir":"","previous_headings":"","what":"Help With Environmental Statistical Methods","title":"Help With Environmental Statistical Methods","text":"goal MDChelp provide functions demonstrations common statistical methods fish/ wildlife research.","code":""},{"path":"https://tlyons253.github.io/MDChelp/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Help With Environmental Statistical Methods","text":"can install development version MDChelp GitHub :","code":"options(download.file.method = \"wininet\") # install.packages(\"pak\") pak::pak(\"tlyons253/MDChelp\")"},{"path":"https://tlyons253.github.io/MDChelp/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Help With Environmental Statistical Methods","text":"basic example shows solve common problem:","code":"library(MDChelp) ## simulate data and estimate abundance using Chapman's version of a Lincoln-Peterson estimator  LP.sim(500,75,0.3,sample.fixed=TRUE)->sim.dat   chapman(sim.dat$r,         sim.dat$n,         sim.dat$m) #> $N.hat #> [1] 373.7586 #>  #> $SE #> [1] 48.04161"},{"path":"https://tlyons253.github.io/MDChelp/reference/LP.sim.html","id":null,"dir":"Reference","previous_headings":"","what":"Lincoln-Petersen Simulation — LP.sim","title":"Lincoln-Petersen Simulation — LP.sim","text":"Simulate data closed, 2-sample mark/recapture study","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/LP.sim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Lincoln-Petersen Simulation — LP.sim","text":"","code":"LP.sim(N, mark, recap, sample.fixed = FALSE)"},{"path":"https://tlyons253.github.io/MDChelp/reference/LP.sim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Lincoln-Petersen Simulation — LP.sim","text":"N Simulated population size mark number individuals marked probability individual caught marked initial sample period. recap probability individual encountered second sample period sample.fixed sample size fixed . TRUE, mark number >1 represents number known marked individuals released, otherwise, probability (0,1)","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/LP.sim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Lincoln-Petersen Simulation — LP.sim","text":"list containing: r number individuals marked first sample period n number individuals second sample. m number previously marked individuals second sample.","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/LP.sim.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Lincoln-Petersen Simulation — LP.sim","text":"","code":"# Two examples if capture is simulated as a fixed number of individuals, or a probability if (FALSE) { # \\dontrun{     LP.sim(N=5E4, mark=300, recap=0.2, sample.fixed=TRUE)->sim1    LP.sim(N=5E4, mark=0.3, recap=0.2, sample.fixed=FALSE)->sim2 } # }"},{"path":"https://tlyons253.github.io/MDChelp/reference/bca_jacknife.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute bias-corrected, accelerated confidence intervals for non-parametric bootstrapped parameter estimates. — bca_jacknife","title":"Compute bias-corrected, accelerated confidence intervals for non-parametric bootstrapped parameter estimates. — bca_jacknife","text":"Compute bias-corrected, accelerated confidence intervals non-parametric bootstrapped parameter estimates.","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/bca_jacknife.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute bias-corrected, accelerated confidence intervals for non-parametric bootstrapped parameter estimates. — bca_jacknife","text":"","code":"bca_jacknife(boot.est, OG.dat, est.fxn, alpha, multcomp = TRUE)"},{"path":"https://tlyons253.github.io/MDChelp/reference/bca_jacknife.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute bias-corrected, accelerated confidence intervals for non-parametric bootstrapped parameter estimates. — bca_jacknife","text":"boot.est data frame n x p rows (n= # bootstrap replicates, p= # model terms parameters two columns: one parameter name (term), one parameter estimate (estimate)) OG.dat orignal data set est.fxn summary function performed data. See est_accelerate detailed description function needs return. alpha desired 2-tail alpha level multcomp true, resulting table adjusted CI's family-wide error rate specified alpha level. FALSE, error rate applied parameter. Correction using Sidak method instituted default. appropriate looking table regression coefficients bootstrapped CI's.","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/bca_jacknife.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute bias-corrected, accelerated confidence intervals for non-parametric bootstrapped parameter estimates. — bca_jacknife","text":"data frame containing parameter name (term), theta.hat, estimate parameter orignal data, lower upper BCa confidence intervals","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/bca_jacknife.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute bias-corrected, accelerated confidence intervals for non-parametric bootstrapped parameter estimates. — bca_jacknife","text":"","code":"# data.frame(X1=seq(1,10,1), #            X2=rnorm(10,0,1))%>% #   mutate(Y=0.2*X1+X2)%>% #   select(-X2)->tmp # # Create bootstrapped data sets # # purrr::map(1:10,~sample(1:nrow(tmp),size=nrow(tmp),replace=TRUE)%>% #               tmp[.,])->boot.dat # # # my.fxn1<-function(X){ #   lm(Y~X1,data=X)%>% #     broom::tidy(.)%>% #     select(term,estimate) # } # # # #this function above could look different every time depending on bootstrap approach # # but should return a 2-column dataframe with columns term- the parameter name, # # and estimate. You do not need to use a function here, I just did for convenience. # # You just need to get the bootstrapped parameter estimates in the correct format. # # # est_boot<-function(est.fxn,boot.list){ # #   map(boot.list,~est.fxn(.x))%>% #     bind_rows()%>% #     rename(theta.boot=estimate)->out # #   return(out) # # } # # # est_boot(my.fxn1,boot.dat)->boot.out # # # bca_jacknife(boot.out,tmp,my.fxn1,alpha=0.05,multcomp = FALSE)"},{"path":"https://tlyons253.github.io/MDChelp/reference/boot_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Resample clustered data — boot_cluster","title":"Resample clustered data — boot_cluster","text":"function defines cluster variables, resamples unique clusters N times, N number unique clusters. Observations within cluster also resampled.","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/boot_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resample clustered data — boot_cluster","text":"","code":"boot_cluster(dat, cluster.vars, out.folder, i)"},{"path":"https://tlyons253.github.io/MDChelp/reference/boot_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resample clustered data — boot_cluster","text":"dat original data bootstrapped (contain variable named \"cluster\"). cluster.vars unquoted vector column names used construct cluster. unquoted vector column names. .folder quoted string gives path existing folder bootstrapped data objects written. counter written name bootstrapped data (e.g bootdat_1.rds, dat_2.rds)","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/boot_cluster.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Resample clustered data — boot_cluster","text":"Writes RDS file containing one bootstrapped data set.","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/chapman.html","id":null,"dir":"Reference","previous_headings":"","what":"Chapman Estimator for closed populations — chapman","title":"Chapman Estimator for closed populations — chapman","text":"Uses Chapman's modified version Lincoln-Petersen estimator two-sample closed population abundance estimator.","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/chapman.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Chapman Estimator for closed populations — chapman","text":"","code":"chapman(r, n, m)"},{"path":"https://tlyons253.github.io/MDChelp/reference/chapman.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Chapman Estimator for closed populations — chapman","text":"r number individuals marked initial sample. n total number individuals (marked unmarked) encountered second sample. m number marked individuals encountered second sample","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/chapman.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Chapman Estimator for closed populations — chapman","text":"list containing: N.hat abundance estimate SE standard error abundance estimate","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/chapman.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Chapman Estimator for closed populations — chapman","text":"","code":"# Use simulation code to generate data and analyze it if (FALSE) { # \\dontrun{    LP.sim(N=5E4, mark=300, recap=0.2,sample.fixed=TRUE)->sim.dat    chapman(r=sim.dat$r,           n=sim.dat$n,           m=sim.dat$m) } # }"},{"path":"https://tlyons253.github.io/MDChelp/reference/est_accelerate.html","id":null,"dir":"Reference","previous_headings":"","what":"Jacknife procedure to estimate the acceleration constant — est_accelerate","title":"Jacknife procedure to estimate the acceleration constant — est_accelerate","text":"function called internal bca_jacknife compute acceleration constant parameter interest.","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/est_accelerate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Jacknife procedure to estimate the acceleration constant — est_accelerate","text":"","code":"est_accelerate(OG.dat, dat.list, est.fxn)"},{"path":"https://tlyons253.github.io/MDChelp/reference/est_accelerate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Jacknife procedure to estimate the acceleration constant — est_accelerate","text":"OG.dat orignal data set, data frame. dat.list list Jacknifed data sets. Generated internally bca_jacknife est.fxn user-supplied function performs desired operation data. Typically call lm, lmer, etc. object can coerced data.frame similar returned broom::tidy (broom.mixed::tidy). Must return data.frame columns \"term\" \"estimate\", representing parameter name, estimate.","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/est_accelerate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Jacknife procedure to estimate the acceleration constant — est_accelerate","text":"list comprised 2 data frames. first data frame first column (term) indicating parameter second column (), acceleration constant term.","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/jacknife_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Jacknife data helper function — jacknife_data","title":"Jacknife data helper function — jacknife_data","text":"function create list jacknifed data objects. Used internally elsewhere available case users wish write jacknifed data objects file","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/jacknife_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Jacknife data helper function — jacknife_data","text":"","code":"jacknife_data(X)"},{"path":"https://tlyons253.github.io/MDChelp/reference/jacknife_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Jacknife data helper function — jacknife_data","text":"X data frame, original data","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/jacknife_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Jacknife data helper function — jacknife_data","text":"list nrow(X) data frames, nrow(X)-1 rows; list jacknifed data sets passed est_accelerate()","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/jacknife_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Jacknife data helper function — jacknife_data","text":"","code":"# data.frame(X1=seq(1,10,1), # X2=rnorm(10,0,1))%>% # mutate(Y=0.2*X1+X2)%>% # select(-X2)->tmp  # jacknife.out<-jacknife_data(tmp)"},{"path":"https://tlyons253.github.io/MDChelp/reference/logexp.html","id":null,"dir":"Reference","previous_headings":"","what":"logistic exposure link function. — logexp","title":"logistic exposure link function. — logexp","text":"logistic exposure link use glm's R sourced Ben Bolker's website : https://rpubs.com/bbolker/logregexp","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/logexp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"logistic exposure link function. — logexp","text":"","code":"logexp(exposure = 1)"},{"path":"https://tlyons253.github.io/MDChelp/reference/logexp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"logistic exposure link function. — logexp","text":"exposure length time. defaults 1 unit.","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/logexp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"logistic exposure link function. — logexp","text":"","code":"# Simulate binomial survival data and estimate dsr  if (FALSE) { # \\dontrun{  # create dummy data n.ind<-30 # number of individuals X intervals dsr<-0.9 # simulated daily survival rate expose<-sample(c(1,2,3),n.ind,replace=TRUE) # simulate the exposure interval length  Y<-rbinom(n.ind,1,dsr^expose) #observed survival  demo.dat<-data.frame(Y=Y,expose=expose)   mod<-glm(Y~1,         family=binomial(link=MDChelp::logexp(demo.dat$expose)),         data=demo.dat)    predict.dat<-data.frame(Y=1,expose=1)    predict(mod,predict.dat,type='link',se.fit=TRUE)  # doesn't work with type='response' and 'newdat'  } # }"},{"path":"https://tlyons253.github.io/MDChelp/reference/resamp.fxn.html","id":null,"dir":"Reference","previous_headings":"","what":"Resampling helper function — resamp.fxn","title":"Resampling helper function — resamp.fxn","text":"Function called internally boot_cluster resample observations within cluster. uses vector resampled \"clusters: (tmp.name), pulls just observations given cluster original data (tmp.dat) returns data frame resampled observations cluster. done repeatedly within boot_cluster results combined produce single bootstrapped data set.","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/resamp.fxn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resampling helper function — resamp.fxn","text":"","code":"resamp.fxn(tmp.dat, tmp.name)"},{"path":"https://tlyons253.github.io/MDChelp/reference/resamp.fxn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resampling helper function — resamp.fxn","text":"tmp.dat data bootstrapped, contains \"cluster\" variable created boot_cluster tmp.name cluster value, used subset data resampling observations within specified cluster.","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/resamp.fxn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Resampling helper function — resamp.fxn","text":"data frame resampled observations within single cluster","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/sim1_logexp.html","id":null,"dir":"Reference","previous_headings":"","what":"A simple logistic exposure survival simulation. — sim1_logexp","title":"A simple logistic exposure survival simulation. — sim1_logexp","text":"simulate logistic exposure survival data, returns \"long\" object n.individuals X n observations rows 'wide' object different analysis method. permits right censoring","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/sim1_logexp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A simple logistic exposure survival simulation. — sim1_logexp","text":"","code":"sim1_logexp(   S.int = 0.95,   nind = 10,   ntime = 10,   p.obs = 0.5,   obs.start = TRUE,   r.censor = FALSE,   p.censor = 0.1,   t.censor = 0.8,   cjs = FALSE,   logexp = TRUE )"},{"path":"https://tlyons253.github.io/MDChelp/reference/sim1_logexp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A simple logistic exposure survival simulation. — sim1_logexp","text":"S.int interval survival probability nind number individuals ntime length encounter histories p.obs probability create differences exposure period obs.start TRUE FALSE, true, individuals observed time =1, otherwise. individuals may observed later encounter history, failing r.censor TRUE FALSE, probability individual right censored. function defaults individuals observed last time interval (typical nest survival studies) p.censor probability individual right censored time = ntime t.censor controls censoring occurs treating censoring time binomial process number trials length encounter history t.censor probability, resulting number successes interval censoring occurs cjs TRUE, encounter history output matrix typical CJS matrix, suitable use bayesian frameowrk logexp TRUE, returns data frame rows individual observation intervals use glm similar","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/sim1_logexp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A simple logistic exposure survival simulation. — sim1_logexp","text":"logexp = TRUE, data frame nest-visits interval visits separate column.","code":""},{"path":"https://tlyons253.github.io/MDChelp/reference/sim1_logexp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A simple logistic exposure survival simulation. — sim1_logexp","text":"","code":"# Show a long-format survival data set using the above defaults if (FALSE) { # \\dontrun{  sim1_logexp(r.censor=TRUE,p.censor=0.2)->sim.dat  } # }"}]
